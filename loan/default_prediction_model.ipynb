{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79985543-77b8-43da-b11d-b1989c6e8f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier \n",
    "from sklearn.svm import SVC \n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d430e20-2ed7-47e2-b149-f0be7265566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_irrelevant_columns(data):\n",
    "    \"\"\"Drops irrelevant or highly missing columns.\"\"\"\n",
    "    drop_cols = ['id', 'member_id', 'url', 'desc', 'next_pymnt_d',\n",
    "                 'annual_inc_joint', 'dti_joint', 'verification_status_joint',\n",
    "                 'mths_since_last_major_derog', 'mths_since_last_delinq',\n",
    "                 'mths_since_last_record', 'tot_coll_amt', 'tot_cur_bal']\n",
    "    \n",
    "    cols_to_drop_existing = [col for col in drop_cols if col in data.columns]\n",
    "    data = data.drop(columns=cols_to_drop_existing)\n",
    "    print(f\"Dropped columns: {cols_to_drop_existing}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740db41-f36b-46d2-b7f6-1c5d93518137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_target_variable(data):\n",
    "    \"\"\"Defines the binary target variable 'loan_default'.\"\"\"\n",
    "    closed_statuses = [\n",
    "        'Fully Paid', 'Charged Off', 'Default',\n",
    "        'Does not meet the credit policy. Status:Fully Paid',\n",
    "        'Does not meet the credit policy. Status:Charged Off'\n",
    "    ]\n",
    "    \n",
    "    # Categorize loan status into default (1) or non-default (0)\n",
    "    data['loan_default'] = data['loan_status'].apply(lambda x: 1 if x in ['Charged Off', 'Default', 'Does not meet the credit policy. Status:Charged Off'] else (0 if x in ['Fully Paid', 'Does not meet the credit policy. Status:Fully Paid', 'Current', 'In Grace Period', 'Late (16-30 days)', 'Late (31-120 days)'] else np.nan))\n",
    "    \n",
    "    # Drop rows where loan_default could not be determined\n",
    "    initial_rows = data.shape[0]\n",
    "    data.dropna(subset=['loan_default'], inplace=True)\n",
    "    print(f\"Dropped {initial_rows - data.shape[0]} rows with undefined 'loan_default' status.\")\n",
    "    \n",
    "    data['loan_default'] = data['loan_default'].astype(int)\n",
    "    print(\"Target variable 'loan_default' created.\")\n",
    "    print(data['loan_default'].value_counts())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daa8408-759e-49eb-9ea0-77b916211de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(data):\n",
    "    \"\"\"Fills missing values based on column type.\"\"\"\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype == 'object':  # Categorical\n",
    "            if data[col].isnull().sum() > 0:\n",
    "                mode_val = data[col].mode()[0]\n",
    "                data[col].fillna(mode_val, inplace=True)\n",
    "                # print(f\"Filled missing values in '{col}' with mode: {mode_val}\")\n",
    "        else: \n",
    "            if data[col].isnull().sum() > 0:\n",
    "                median_val = data[col].median()\n",
    "                data[col].fillna(median_val, inplace=True)\n",
    "                # print(f\"Filled missing values in '{col}' with median: {median_val}\")\n",
    "    print(\"Missing values handled.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c44cd-6881-4e2c-9cbc-6eea608575a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers(data, numerical_cols):\n",
    "    \"\"\"Caps numerical outliers at the 99th percentile.\"\"\"\n",
    "    for col in numerical_cols:\n",
    "        if col in data.columns:\n",
    "            upper_bound = data[col].quantile(0.99)\n",
    "            data[col] = np.where(data[col] > upper_bound, upper_bound, data[col])\n",
    "            # print(f\"Capped outliers in '{col}' at 99th percentile.\")\n",
    "    print(\"Outliers capped for numerical columns.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c350a7-e727-492b-a3da-148327e5135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_financial_ratios(data):\n",
    "    \"\"\"Creates loan-to-income and credit utilization ratios.\"\"\"\n",
    "    if 'loan_amnt' in data.columns and 'annual_inc' in data.columns:\n",
    "        data['loan_to_income_ratio'] = data['loan_amnt'] / (data['annual_inc'] + 1e-6)\n",
    "    else:\n",
    "        print(\"Warning: 'loan_amnt' or 'annual_inc' not found. Skipping 'loan_to_income_ratio' calculation.\")\n",
    "        data['loan_to_income_ratio'] = np.nan\n",
    "\n",
    "    if 'revol_hi_lim' in data.columns and 'revol_bal' in data.columns:\n",
    "        data['revol_hi_lim'] = data['revol_hi_lim'].replace(0, np.nan)\n",
    "        data['revol_hi_lim'].fillna(data['revol_hi_lim'].median(), inplace=True)\n",
    "        data['credit_util_ratio'] = data['revol_bal'] / (data['revol_hi_lim'] + 1e-6)\n",
    "        print(\"Financial ratios created.\")\n",
    "    else:\n",
    "        print(\"Warning: 'revol_hi_lim' or 'revol_bal' not found. Skipping 'credit_util_ratio' calculation.\")\n",
    "        data['credit_util_ratio'] = np.nan\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f4b914-89ae-4d34-a0f8-f786fbccf668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_features(data):\n",
    "    \"\"\"Extracts year and month from date columns and calculates credit history length.\"\"\"\n",
    "    date_cols = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d']\n",
    "    for col in date_cols:\n",
    "        # Convert only existing date columns\n",
    "        if col in data.columns:\n",
    "            # Handle mixed date formats by coercing errors\n",
    "            data[col] = pd.to_datetime(data[col], format='%b-%y', errors='coerce') \n",
    "            data[f'{col}_month'] = data[col].dt.month\n",
    "            data[f'{col}_year'] = data[col].dt.year\n",
    "            # Drop the original date column after extracting features\n",
    "            data = data.drop(columns=[col])\n",
    "    \n",
    "    # Calculate credit history length, handling potential NaNs from date conversion\n",
    "    if 'issue_d_year' in data.columns and 'earliest_cr_line_year' in data.columns:\n",
    "        data['credit_history_length'] = data['issue_d_year'] - data['earliest_cr_line_year']\n",
    "        data['credit_history_length'].fillna(data['credit_history_length'].median(), inplace=True) # Impute any NaNs\n",
    "    print(\"Date-based features extracted.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f951b6-cd11-45ba-a825-dd7fa4f1909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_numeric(data):\n",
    "    \"\"\"Converts string-based numeric columns to actual numeric types.\"\"\"\n",
    "    if 'emp_length' in data.columns:\n",
    "        emp_length_map = {\n",
    "            '10+ years': 10, '9 years': 9, '8 years': 8, '7 years': 7, '6 years': 6,\n",
    "            '5 years': 5, '4 years': 4, '3 years': 3, '2 years': 2, '1 year': 1,\n",
    "            '< 1 year': 0.5, 'n/a': np.nan\n",
    "        }\n",
    "        data['emp_length_numeric'] = data['emp_length'].replace(emp_length_map).astype(float)\n",
    "        data['emp_length_numeric'].fillna(data['emp_length_numeric'].median(), inplace=True)\n",
    "        data = data.drop(columns=['emp_length'])\n",
    "    else:\n",
    "        print(\"Warning: 'emp_length' not found. Skipping 'emp_length_numeric' conversion.\")\n",
    "\n",
    "    if 'term' in data.columns:\n",
    "        data['term_numeric'] = data['term'].astype(str).str.extract('(\\\\d+)').astype(float)\n",
    "        data = data.drop(columns=['term'])\n",
    "    else:\n",
    "        print(\"Warning: 'term' not found. Skipping 'term_numeric' conversion.\")\n",
    "\n",
    "    if 'int_rate' in data.columns:\n",
    "        data['int_rate_numeric'] = data['int_rate'].astype(str).str.replace('%', '', regex=False).astype(float)\n",
    "        data = data.drop(columns=['int_rate'])\n",
    "    else:\n",
    "        print(\"Warning: 'int_rate' not found. Skipping 'int_rate_numeric' conversion.\")\n",
    "\n",
    "    if 'revol_util' in data.columns:\n",
    "        data['revol_util_numeric'] = data['revol_util'].astype(str).str.replace('%', '', regex=False).astype(float)\n",
    "        data = data.drop(columns=['revol_util'])\n",
    "    else:\n",
    "        print(\"Warning: 'revol_util' not found. Skipping 'revol_util_numeric' conversion.\")\n",
    "        \n",
    "    print(\"String-based numeric features converted.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0984c-f1bf-4545-80c3-34be0fc0a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_feature_types(data, exclude_cols=[]):\n",
    "    \"\"\"Identifies numerical and categorical columns.\"\"\"\n",
    "    numerical_cols = data.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_cols = data.select_dtypes(include='object').columns.tolist()\n",
    "    \n",
    "    # Exclude target variable and any other specified columns\n",
    "    if 'loan_default' in numerical_cols:\n",
    "        numerical_cols.remove('loan_default')\n",
    "    \n",
    "    numerical_cols = [col for col in numerical_cols if col not in exclude_cols]\n",
    "    categorical_cols = [col for col in categorical_cols if col not in exclude_cols]\n",
    "\n",
    "    print(f\"Identified {len(numerical_cols)} numerical and {len(categorical_cols)} categorical columns.\")\n",
    "    return numerical_cols, categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be23f9ad-be44-49c7-8692-d61663eef71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessing_pipeline(numerical_cols, categorical_cols):\n",
    "    \"\"\"Creates a preprocessing pipeline using ColumnTransformer.\"\"\"\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "        ],\n",
    "        remainder='passthrough' \n",
    "    )\n",
    "    print(\"Preprocessing pipeline created.\")\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4451c-1d23-4c09-9a8a-d3cee661c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_woe_iv(data, feature, target):\n",
    "    \"\"\"\n",
    "    Calculates Weight of Evidence (WoE) and Information Value (IV) for a given feature.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing the feature and target columns.\n",
    "        feature (str): The name of the feature column.\n",
    "        target (str): The name of the binary target column (0s and 1s).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - woe_df (pd.DataFrame): DataFrame with WoE and IV_Contrib for each category.\n",
    "            - iv (float): Total Information Value for the feature.\n",
    "    \"\"\"\n",
    "    if not all(data[target].isin([0, 1])):\n",
    "        raise ValueError(f\"Target column '{target}' must contain only 0s and 1s.\")\n",
    "    \n",
    "    # Ensure the feature is treated as categorical for grouping\n",
    "    df_temp = data.copy()\n",
    "    df_temp[feature] = df_temp[feature].astype(str) # Convert to string to ensure correct grouping for all types\n",
    "\n",
    "    grouped = df_temp.groupby(feature)[target].agg(['count', 'sum'])\n",
    "    grouped.columns = ['Total', 'Bad']  # 'Bad' refers to the positive class (default)\n",
    "    grouped['Good'] = grouped['Total'] - grouped['Bad'] # 'Good' refers to the negative class (non-default)\n",
    "    \n",
    "    total_good = grouped['Good'].sum()\n",
    "    total_bad = grouped['Bad'].sum()\n",
    "\n",
    "    if total_good == 0 or total_bad == 0:\n",
    "        print(f\"Warning: Total good or bad cases are zero for feature '{feature}'. IV will be 0.\")\n",
    "        return pd.DataFrame(), 0.0\n",
    "\n",
    "    grouped['Good_Pct'] = grouped['Good'] / total_good\n",
    "    grouped['Bad_Pct'] = grouped['Bad'] / total_bad\n",
    "\n",
    "    epsilon = 0.000001\n",
    "    grouped['WoE'] = np.log((grouped['Good_Pct'] + epsilon) / (grouped['Bad_Pct'] + epsilon))\n",
    "    grouped['IV_Contrib'] = (grouped['Good_Pct'] - grouped['Bad_Pct']) * grouped['WoE']\n",
    "    iv = grouped['IV_Contrib'].sum()\n",
    "\n",
    "    woe_df = grouped.reset_index()\n",
    "    woe_df = woe_df[[feature, 'Good', 'Bad', 'Good_Pct', 'Bad_Pct', 'WoE', 'IV_Contrib']]\n",
    "    woe_df.rename(columns={feature: 'Category'}, inplace=True)\n",
    "    woe_df['Feature'] = feature\n",
    "\n",
    "    return woe_df, iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd7038-6465-45ee-8bf2-c2ab66b19070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_woe_transformation(data_to_transform, woe_map, feature_name, new_col_name=None):\n",
    "    \"\"\"\n",
    "    Applies WoE transformation to a specified feature in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data_to_transform (pd.DataFrame): The DataFrame to transform.\n",
    "        woe_map (dict): A dictionary mapping feature categories to their WoE values.\n",
    "        feature_name (str): The name of the original feature column.\n",
    "        new_col_name (str, optional): The name for the new WoE transformed column.\n",
    "                                      Defaults to f'{feature_name}_WoE'.\n",
    "                                      \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the new WoE transformed column.\n",
    "    \"\"\"\n",
    "    if new_col_name is None:\n",
    "        new_col_name = f'{feature_name}_WoE'\n",
    "    \n",
    "    temp_series = data_to_transform[feature_name].astype(str)\n",
    "    \n",
    "    # Apply the WoE mapping\n",
    "    data_to_transform[new_col_name] = temp_series.map(woe_map).fillna(0) \n",
    "    return data_to_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97766a70-46e1-421a-b71f-1be9a5695af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_csi(expected, actual):\n",
    "    \"\"\"Calculates CSI for two pandas Series.\"\"\"\n",
    "    all_categories = pd.concat([expected, actual]).unique()\n",
    "    expected_dist = expected.value_counts(normalize=True).reindex(all_categories, fill_value=0).sort_index()\n",
    "    actual_dist = actual.value_counts(normalize=True).reindex(all_categories, fill_value=0).sort_index()\n",
    "\n",
    "    epsilon = 0.000001\n",
    "    expected_dist = expected_dist.replace(0, epsilon)\n",
    "    actual_dist = actual_dist.replace(0, epsilon)\n",
    "\n",
    "    csi = ((expected_dist - actual_dist) * np.log(expected_dist / actual_dist)).sum()\n",
    "    return csi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a717b858-f4c2-4c53-b126-09d6c6839f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('car.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d4dc8-3754-4f98-ac9d-db25a85f0d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387acd4-ddf0-47ef-8e13-887a9f34d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f39a4-9fac-496a-b4c8-1380216054c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94c0628-2c7f-4440-a54b-aea1494c666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbef1a6-ebbd-45d7-b8e5-ce2b12c097b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data cleaning\n",
    "df_cleaned = drop_irrelevant_columns(df.copy())\n",
    "df_cleaned = define_target_variable(df_cleaned.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56c6cb-b5ae-475f-96b2-383a5bd1d3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'loan_status' column after creating 'loan_default'\n",
    "if 'loan_status' in df_cleaned.columns:\n",
    "        df_cleaned = df_cleaned.drop(columns=['loan_status'])\n",
    "    \n",
    "df_cleaned = handle_missing_values(df_cleaned.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210b663-15a9-4773-aeb9-eded74682b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering\n",
    "df_fe = create_financial_ratios(df_cleaned.copy())\n",
    "df_fe = extract_date_features(df_fe.copy())\n",
    "df_fe = convert_string_to_numeric(df_fe.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d379657-dce4-4257-9df3-094527610e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns after feature engineering and conversions\n",
    "numerical_features_all, categorical_features_all = identify_feature_types(df_fe.copy(), exclude_cols=['loan_default'])\n",
    "\n",
    "# Cap outliers using the updated list of numerical features\n",
    "df_fe = cap_outliers(df_fe.copy(), numerical_features_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74986b07-873e-4428-9bd6-6eb86dfc20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df_fe.drop('loan_default', axis=1)\n",
    "y = df_fe['loan_default']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"\\nData split into training (X_train shape: {X_train.shape}) and testing (X_test shape: {X_test.shape}) sets.\")\n",
    "print(f\"Target distribution in training: {y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Target distribution in testing: {y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b320ed95-ecd0-4e53-988f-dc0baf432450",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- CSI Report (Before Advanced Preprocessing) ---\")\n",
    "csi_report = []\n",
    "all_features_for_csi = list(set(X_train.columns) | set(X_test.columns))\n",
    "\n",
    "for col in all_features_for_csi:\n",
    "    if col in X_train.columns and col in X_test.columns:\n",
    "        csi_val = calculate_csi(X_train[col].astype(str), X_test[col].astype(str))\n",
    "        csi_report.append({'Feature': col, 'CSI': round(csi_val, 4)})\n",
    "csi_df = pd.DataFrame(csi_report).sort_values(by='CSI', ascending=False)\n",
    "print(csi_df)\n",
    "print(\"\\n--- End of CSI Report ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8958a63b-5860-4552-93ee-22ef6efb731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with CSI less than 0.1\n",
    "cols_to_drop_by_csi = csi_df[csi_df['CSI'] < 0.1]['Feature'].tolist()\n",
    "X_train = X_train.drop(columns=cols_to_drop_by_csi, errors='ignore')\n",
    "X_test = X_test.drop(columns=cols_to_drop_by_csi, errors='ignore')\n",
    "print(f\"Dropped {len(cols_to_drop_by_csi)} columns with CSI less than 0.1: {cols_to_drop_by_csi}\")\n",
    "print(f\"New X_train shape after CSI-based dropping: {X_train.shape}\")\n",
    "print(f\"New X_test shape after CSI-based dropping: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c4142-3215-4f1c-9db0-b1ca0f7f5686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WoE and IV Calculation ---\n",
    "print(\"\\n--- Calculating WoE and IV for selected features ---\")\n",
    "\n",
    "# Combine X_train and y_train for WoE calculation\n",
    "train_for_woe_iv = X_train.copy()\n",
    "train_for_woe_iv['loan_default'] = y_train\n",
    "\n",
    "woe_maps = {} \n",
    "iv_summary = []\n",
    "woe_data_dfs = [] \n",
    "\n",
    "# Identify features for WoE/IV calculation\n",
    "features_for_woe_iv_calculation = []\n",
    "\n",
    "# Add categorical features (excluding high cardinality ones if desired, though current code doesn't filter by cardinality)\n",
    "for col in categorical_features_all:\n",
    "    if col in train_for_woe_iv.columns: \n",
    "        features_for_woe_iv_calculation.append(col)\n",
    "        \n",
    "# Add low-cardinality numerical features\n",
    "for col in numerical_features_all: \n",
    "    if col in train_for_woe_iv.columns and train_for_woe_iv[col].nunique() < 20:\n",
    "         if col != 'loan_default': \n",
    "            features_for_woe_iv_calculation.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abfde5a-9d28-422d-ae2b-b854d8d5d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special handling for 'annual_inc': Bin it for WoE calculation\n",
    "annual_inc_binned_col = 'annual_inc_binned'\n",
    "annual_inc_bins = None # Initialize to None\n",
    "if 'annual_inc' in X_train.columns:\n",
    "    try:\n",
    "        train_for_woe_iv[annual_inc_binned_col], annual_inc_bins = pd.qcut(train_for_woe_iv['annual_inc'], \n",
    "                                                                            q=5, duplicates='drop', \n",
    "                                                                            labels=False, # Use integer labels\n",
    "                                                                            retbins=True)\n",
    "        X_train[annual_inc_binned_col] = pd.cut(X_train['annual_inc'], bins=annual_inc_bins, \n",
    "                                                 labels=False, include_lowest=True, right=True, \n",
    "                                                 duplicates='drop')\n",
    "        features_for_woe_iv_calculation.append(annual_inc_binned_col)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not bin 'annual_inc' for WoE calculation: {e}\")\n",
    "        if annual_inc_binned_col in features_for_woe_iv_calculation:\n",
    "            features_for_woe_iv_calculation.remove(annual_inc_binned_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98cf32-a0c8-41f7-a0bd-8c98894386da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up features_for_woe_iv_calculation to remove potential duplicates or problematic columns\n",
    "features_for_woe_iv_calculation = list(set(features_for_woe_iv_calculation))\n",
    "\n",
    "# Perform WoE/IV calculation\n",
    "for feature in features_for_woe_iv_calculation:\n",
    "    print(f\"Calculating WoE and IV for: {feature}\")\n",
    "    try:\n",
    "        woe_df, iv = calculate_woe_iv(train_for_woe_iv, feature, 'loan_default')\n",
    "        iv_summary.append({'Feature': feature, 'IV': iv})\n",
    "        if not woe_df.empty:\n",
    "            woe_data_dfs.append(woe_df)\n",
    "            # Store WoE mapping for later transformation\n",
    "            woe_map = dict(zip(woe_df['Category'].astype(str), woe_df['WoE']))\n",
    "            woe_maps[feature] = woe_map\n",
    "        print(f\"  IV for {feature}: {iv:.4f}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"  Error calculating WoE/IV for {feature}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  An unexpected error occurred for {feature}: {e}\")\n",
    "\n",
    "if iv_summary:\n",
    "    iv_summary_df = pd.DataFrame(iv_summary).sort_values(by='IV', ascending=False)\n",
    "    print(\"\\n--- Information Value (IV) Summary ---\")\n",
    "    print(iv_summary_df)\n",
    "\n",
    "    print(\"\\n--- WoE Details for each Feature ---\")\n",
    "    if woe_data_dfs:\n",
    "        full_woe_df = pd.concat(woe_data_dfs, ignore_index=True)\n",
    "        print(full_woe_df)\n",
    "    else:\n",
    "        print(\"No WoE details generated.\")\n",
    "else:\n",
    "    print(\"No IV summary available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2000c9eb-6ed9-4196-8ee0-5fc4eab320d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Apply WoE Transformation ---\n",
    "print(\"\\n--- Applying WoE Transformation ---\")\n",
    "woe_transformed_cols = []\n",
    "\n",
    "# Handle annual_inc binning for X_test before applying WoE, using bins from X_train\n",
    "if 'annual_inc' in X_test.columns and annual_inc_binned_col in features_for_woe_iv_calculation and annual_inc_bins is not None:\n",
    "    X_test[annual_inc_binned_col] = pd.cut(X_test['annual_inc'], bins=annual_inc_bins, \n",
    "                                             labels=False, include_lowest=True, right=True, \n",
    "                                             duplicates='drop')\n",
    "    X_test[annual_inc_binned_col].fillna(X_train[annual_inc_binned_col].mode()[0], inplace=True)\n",
    "    X_train[annual_inc_binned_col].fillna(X_train[annual_inc_binned_col].mode()[0], inplace=True)\n",
    "\n",
    "\n",
    "for feature, woe_map in woe_maps.items():\n",
    "    # Apply WoE to training set\n",
    "    X_train = apply_woe_transformation(X_train, woe_map, feature)\n",
    "    # Apply WoE to test set\n",
    "    X_test = apply_woe_transformation(X_test, woe_map, feature)\n",
    "    woe_transformed_cols.append(f'{feature}_WoE')\n",
    "    print(f\"  Applied WoE for '{feature}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28359cd0-b68c-4b7f-943c-ba3f66c6a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Update Feature Lists for ColumnTransformer ---\n",
    "# Numerical features now include the new WoE transformed columns\n",
    "numerical_features_for_pipeline = [col for col in numerical_features_all if col not in features_for_woe_iv_calculation] + woe_transformed_cols\n",
    "\n",
    "categorical_features_for_pipeline = [col for col in categorical_features_all if col not in features_for_woe_iv_calculation]\n",
    "\n",
    "X_train = X_train.drop(columns=features_for_woe_iv_calculation, errors='ignore')\n",
    "X_test = X_test.drop(columns=features_for_woe_iv_calculation, errors='ignore')\n",
    "\n",
    "numerical_features_for_pipeline = [col for col in numerical_features_for_pipeline if col in X_train.columns]\n",
    "categorical_features_for_pipeline = [col for col in categorical_features_for_pipeline if col in X_train.columns]\n",
    "\n",
    "print(f\"\\nFeatures for StandardScaler: {numerical_features_for_pipeline}\")\n",
    "print(f\"Features for OneHotEncoder: {categorical_features_for_pipeline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062540d2-d3c3-4a19-9b33-0c60f1ae6572",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = create_preprocessing_pipeline(numerical_features_for_pipeline, categorical_features_for_pipeline)\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "transformed_feature_names = []\n",
    "transformed_feature_names.extend(numerical_features_for_pipeline)\n",
    "\n",
    "# if 'cat' in [name for name, _, _ in preprocessor.transformers]:\n",
    "#     # Access the fitted OneHotEncoder from the preprocessor's transformers_\n",
    "#     ohe_transformer_index = [i for i, (name, _, _) in enumerate(preprocessor.transformers_) if name == 'cat'][0]\n",
    "#     ohe_transformer = preprocessor.transformers_[ohe_transformer_index][1]\n",
    "#     ohe_feature_names = ohe_transformer.get_feature_names_out(categorical_features_for_pipeline)\n",
    "#     transformed_feature_names.extend(list(ohe_feature_names))\n",
    "\n",
    "X_train_df_final = pd.DataFrame(X_train_processed, index=X_train.index)\n",
    "X_test_df_final = pd.DataFrame(X_test_processed, index=X_test.index)\n",
    "\n",
    "print(\"\\nApplying SMOTE to training data...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_df_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5615cc-f722-40d7-aa0b-7274778850f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Training and Evaluating Multiple Models ---\")\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, solver='liblinear', random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    \"SVC (Linear Kernel)\": SVC(kernel='linear', probability=True, random_state=42, max_iter=1000), # probability=True for ROC AUC\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"XGBoost\": xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "    try:\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "        print(f\"{name} trained successfully.\")\n",
    "\n",
    "        y_pred_test = model.predict(X_test_df_final)\n",
    "        test_acc = accuracy_score(y_test, y_pred_test) * 100\n",
    "        \n",
    "        print(f\"\\n--- {name} Test Set Evaluation ---\")\n",
    "        print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "        print(\"Classification Report (Test):\\n\", classification_report(y_test, y_pred_test))\n",
    "        print(\"Confusion Matrix (Test):\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_proba_test = model.predict_proba(X_test_df_final)[:, 1]\n",
    "            test_roc_auc = roc_auc_score(y_test, y_proba_test)\n",
    "            print(f\"ROC AUC Score (Test): {test_roc_auc:.4f}\")\n",
    "\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_proba_test)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {test_roc_auc:.2f})')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f'ROC Curve for {name}')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"ROC AUC Score not available for {name} (model does not have predict_proba).\")\n",
    "        \n",
    "        results[name] = {\n",
    "            \"Accuracy\": test_acc,\n",
    "            \"Classification Report\": classification_report(y_test, y_pred_test, output_dict=True),\n",
    "            \"Confusion Matrix\": confusion_matrix(y_test, y_pred_test).tolist(),\n",
    "            \"ROC AUC\": test_roc_auc if hasattr(model, \"predict_proba\") else \"N/A\"\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training/evaluation of {name}: {e}\")\n",
    "        results[name] = {\"Error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33123b-7b31-432f-92e8-7c93ab94b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- All Models Evaluation Summary ---\")\n",
    "for name, res in results.items():\n",
    "    print(f\"\\nModel: {name}\")\n",
    "    if \"Error\" in res:\n",
    "        print(f\"  Error: {res['Error']}\")\n",
    "    else:\n",
    "        print(f\"  Test Accuracy: {res['Accuracy']:.2f}%\")\n",
    "        print(f\"  Test ROC AUC: {res['ROC AUC']:.4f}\" if res['ROC AUC'] != \"N/A\" else f\"  Test ROC AUC: {res['ROC AUC']}\")\n",
    "        print(f\"  Test Precision (Class 1): {res['Classification Report']['1']['precision']:.2f}\")\n",
    "        print(f\"  Test Recall (Class 1): {res['Classification Report']['1']['recall']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c2ef0-9715-446d-b7e0-80e5a2499b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
